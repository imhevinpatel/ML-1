{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CAdsFD9JoRz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq05VuOOKXy0"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def backward(self, updated_W_Goutput, lr_alpha):\n",
        "    raise NotImplementedError\n",
        "\n",
        "class Linear_Layer(Layer):\n",
        "  def __init__(self, input_dimensions, no_of_nodes):\n",
        "    super().__init__()\n",
        "    np.random.seed(1234)\n",
        "    self.weights = np.random.randn(input_dimensions, no_of_nodes)\n",
        "    self.bias = np.random.randn(1, no_of_nodes)\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    self.output = np.dot(self.input, self.weights) + self.bias\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, updated_W_Goutput, lr_alpha):\n",
        "    local_derivatives = np.dot(updated_W_Goutput, self.weights.T)\n",
        "    weights_derivatives = np.dot(self.input.T, updated_W_Goutput)\n",
        "    self.weights -= lr_alpha * weights_derivatives\n",
        "    self.bias -= lr_alpha * updated_W_Goutput\n",
        "    return local_derivatives\n",
        "\n",
        "class Sigmoid_Layer(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    self.output = 1 / (1 + np.exp(-input))\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, updated_W_Goutput, lr_alpha):\n",
        "    local_derivatives = self.output * (1 - self.output)\n",
        "    lz_multiplication = updated_W_Goutput * local_derivatives\n",
        "    return lz_multiplication\n",
        "\n",
        "\n",
        "class HyperbolicTangent_Layer(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    self.output = np.tanh(input)\n",
        "    return self.output\n",
        "  \n",
        "  def backward(self, updated_W_Goutput, lr_alpha):\n",
        "    local_derivatives = 1 - np.power(self.output, 2)\n",
        "    lz_multiplication = updated_W_Goutput * local_derivatives\n",
        "    return lz_multiplication\n",
        "\n",
        "class Softmax_Layer(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    tmp = np.exp(input)\n",
        "    self.output = tmp / np.sum(tmp)\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, updated_W_Goutput, lr_alpha):\n",
        "    return updated_W_Goutput\n",
        "\n",
        "\n",
        "class CrossEntropy_Layer(Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def cross_entropy(self, pred_y, actual_y):\n",
        "    loss = -np.sum(actual_y * np.log(pred_y))\n",
        "    return loss\n",
        "\n",
        "  def forward(self, input, actual_y):\n",
        "    self.input = input\n",
        "    self.actual_y = actual_y\n",
        "    self.output = self.cross_entropy(input, actual_y)\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, updated_W_Goutput, lr_alpha):\n",
        "    local_derivatives = self.input - self.actual_y\n",
        "    dl_dy = updated_W_Goutput * local_derivatives\n",
        "    return dl_dy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_split(X_train,Y_train):\n",
        "  sample_num = X_train.shape[0]\n",
        "  random_split = int(0.9 * sample_num)\n",
        "  x_train = X_train[:random_split]\n",
        "  x_val = X_train[random_split:]\n",
        "  y_train = Y_train[:random_split]\n",
        "  y_val =  Y_train[random_split:]\n",
        "\n",
        "  return x_train,x_val,y_train,y_val\n"
      ],
      "metadata": {
        "id": "VcRtBa9IQfJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si0NJFGpT4yC"
      },
      "outputs": [],
      "source": [
        "class Sequential_Layer(Layer):\n",
        "  def __init__(self):\n",
        "    self.layers = []\n",
        "    self.error_list = []\n",
        "    self.val_error_list = []\n",
        "  \n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  def fit(self, X_train, Y_train, epochs, lr_alpha, patience):\n",
        "    cr = CrossEntropy_Layer()\n",
        "\n",
        "    x_train,x_val,y_train,y_val = validation_split(X_train,Y_train)\n",
        "\n",
        "    # Intilization\n",
        "    num_improvements = 0\n",
        "    best_loss = float(\"inf\")\n",
        "    sample_n = x_train.shape[0]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      training_error = 0\n",
        "      val_error = 0\n",
        "      for i in range(sample_n):\n",
        "        output = x_train[i]\n",
        "        # Forward propagation\n",
        "        for layer in self.layers:\n",
        "          output = layer.forward(output)\n",
        "        training_error += cr.forward(output, y_train[i])\n",
        "\n",
        "        # Backward propagation\n",
        "        loss_derivative = cr.backward(1, lr_alpha)\n",
        "        for layer in reversed(self.layers):\n",
        "          loss_derivative = layer.backward(loss_derivative, lr_alpha)\n",
        "\n",
        "        # Prediction using validation data\n",
        "        y_val_predicted = self.predict(x_val)\n",
        "\n",
        "        # Calculating loss\n",
        "        if y_train.shape[1]>1:\n",
        "          val_error += cr.forward(y_val_predicted,np.argmax(y_val, axis = 1))\n",
        "          val_error = val_error/y_val_predicted.shape[0]\n",
        "        else :\n",
        "          val_error += cr.forward(y_val_predicted, y_val)\n",
        "          val_error = val_error/y_val_predicted.shape[0]\n",
        "\n",
        "        # Checking for patience\n",
        "        if val_error <= best_loss:\n",
        "          best_loss = val_error\n",
        "          num_improvements = 0\n",
        "        else:\n",
        "          num_improvements +=1\n",
        "        if num_improvements == patience:\n",
        "          break\n",
        "\n",
        "      # Average error for all samples per epoch\n",
        "      training_error /= sample_n\n",
        "      self.val_error_list.append(best_loss)\n",
        "      self.error_list.append(training_error)\n",
        "\n",
        "  def predict(self, input_data, prob=False):\n",
        "    samples = input_data.shape[0]\n",
        "    result = []\n",
        "    y_pred = []\n",
        "    for i in range(samples):\n",
        "        output = input_data[i]\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        result.append(output)\n",
        "    if prob:\n",
        "        return result\n",
        "\n",
        "    if len(result[0][0])==1:\n",
        "        for i in result:\n",
        "            if abs(i[0][0]) >= 0.5:\n",
        "                y_pred.append(1)\n",
        "            else:\n",
        "                y_pred.append(0) \n",
        "        return np.array(y_pred)\n",
        "    for i in result:\n",
        "        y_pred.append(np.argmax(i[0], axis = 0))\n",
        "    return np.array(y_pred)\n",
        "\n",
        "  def accuracy(elf,y_pred, y_true):\n",
        "    correct = 0\n",
        "    for i in range(len(y_true)):\n",
        "        if y_true[i] == y_pred[i]:\n",
        "            correct += 1\n",
        "    accuracy = correct / float(len(y_true)) * 100.0\n",
        "    return accuracy\n",
        "\n",
        "  def plot(self, lr):\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(self.error_list, 'b')\n",
        "    plt.title(\"Loss vs Epochs\", fontsize=18)\n",
        "    plt.xlabel(\"Epochs\",fontsize=18)\n",
        "    plt.ylabel(\"loss\", fontsize=18)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj7cXLEDUh2X"
      },
      "outputs": [],
      "source": [
        "# Testing model 1 on XOR\n",
        "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]], [[1,0]], [[0,0]], [[0,1]], [[1,0]]])\n",
        "y_train = np.array([[[0]], [[1]], [[1]], [[0]], [[1]],[[0]], [[1]], [[1]]])\n",
        "\n",
        "# Network\n",
        "net = Sequential_Layer()\n",
        "net.add(Linear_Layer(2, 5))\n",
        "net.add(HyperbolicTangent_Layer())\n",
        "net.add(Linear_Layer(5, 1))\n",
        "net.add(HyperbolicTangent_Layer())\n",
        "\n",
        "# Train the network\n",
        "learning_rate = 0.1\n",
        "net.fit(x_train, y_train, epochs=10, lr_alpha = learning_rate, patience=5)\n",
        "y_pred = net.predict(x_train)\n",
        "print(\"Prediction: \",y_pred)\n",
        "print(\"Accuracy :\", net.accuracy(y_pred, y_train),\"%\")\n",
        "\n",
        "# Save Weights\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "\n",
        "my_file_name = \"XOR_solved.w\"\n",
        "with open(my_file_name, 'wb') as myfile:\n",
        "  pkl.dump(net, myfile)\n",
        "\n",
        "# Load the model\n",
        "with open(my_file_name, 'rb') as input_file:\n",
        "  model1 = pkl.load(input_file)\n",
        "\n",
        "net.plot(learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASp17weWVUy6"
      },
      "outputs": [],
      "source": [
        "# Testing model 2 on XOR\n",
        "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]], [[1,0]], [[0,0]], [[0,1]], [[1,0]]])\n",
        "y_train = np.array([[[0]], [[1]], [[1]], [[0]], [[1]],[[0]], [[1]], [[1]]])\n",
        "\n",
        "# Network\n",
        "net = Sequential_Layer()\n",
        "net.add(Linear_Layer(2, 4))\n",
        "net.add(HyperbolicTangent_Layer())\n",
        "net.add(Linear_Layer(4, 1))\n",
        "net.add(HyperbolicTangent_Layer())\n",
        "\n",
        "# Train the network\n",
        "learning_rate = 0.01\n",
        "net.fit(x_train, y_train, epochs=200, lr_alpha=learning_rate, patience=5)\n",
        "y_pred = net.predict(x_train)\n",
        "print(\"Prediction: \",y_pred)\n",
        "print(\"Accuracy :\", net.accuracy(y_pred, y_train),\"%\")\n",
        "\n",
        "# Save Weights\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "\n",
        "my_file_name = \"XOR_solved.w\"\n",
        "with open(my_file_name, 'wb') as myfile:\n",
        "  pkl.dump(net, myfile)\n",
        "\n",
        "\n",
        "net.plot(learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZX7KnTKXmCG"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "def OneHotEncoder(target, num_classes):\n",
        "  res = np.eye(num_classes)[np.array(target).reshape(-1)]\n",
        "  return res.reshape(list(target.shape)+[num_classes])\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 784)\n",
        "x_train = x_train.astype(\"float32\")\n",
        "x_train /= 255\n",
        "y_train = OneHotEncoder(y_train.astype(int), 10)\n",
        "\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 784)\n",
        "x_test = x_test.astype(\"float32\")\n",
        "x_test /= 255\n",
        "y_test = OneHotEncoder(y_test.astype(int), 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYj8VxEqXrkc"
      },
      "outputs": [],
      "source": [
        "def testing_images(model, samples=10):\n",
        "  for test_data, true in zip(x_test[:samples], y_test[:samples]):\n",
        "    image = np.reshape(test_data, (28, 28))\n",
        "    plt.imshow(image, cmap='binary')\n",
        "    prediction = model.predict(test_data)\n",
        "    index = np.argmax(true)\n",
        "    plt.title('Prediction: %s, True: %d' % (prediction[0], index))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "oNVgtN1YYBOT",
        "outputId": "814aaf74-dbd5-461e-9cc6-18deed3b8ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-c3cd7fe82d4d>:79: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = -np.sum(actual_y * np.log(pred_y))\n",
            "<ipython-input-2-c3cd7fe82d4d>:79: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = -np.sum(actual_y * np.log(pred_y))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-54b18f2bd039>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSoftmax_Layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-264695df52bf>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, Y_train, epochs, lr_alpha, patience)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Prediction using validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0my_val_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Calculating loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-264695df52bf>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_data, prob)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c3cd7fe82d4d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Model 1\n",
        "net = Sequential_Layer()\n",
        "net.add(Linear_Layer(784, 10))     \n",
        "net.add(HyperbolicTangent_Layer()) \n",
        "net.add(Softmax_Layer())\n",
        "lr = 0.001\n",
        "net.fit(x_train, y_train, epochs=2, lr_alpha=lr, patience =5)\n",
        "net.plot(learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBz_dOc_Ykww"
      },
      "outputs": [],
      "source": [
        "# Model 2\n",
        "net = Sequential_Layer()\n",
        "net.add(Linear_Layer(784, 10))     \n",
        "net.add(Sigmoid_Layer())\n",
        "net.add(Softmax_Layer())\n",
        "lr = 0.01\n",
        "net.fit(x_train, y_train, epochs=5, lr_alpha=lr, patience =5)\n",
        "net.plot(learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_RrID06YnK0"
      },
      "outputs": [],
      "source": [
        "# Model 3\n",
        "net = Sequential_Layer()\n",
        "net.add(Linear_Layer(784, 10))     \n",
        "net.add(HyperbolicTangent_Layer())\n",
        "net.add(Softmax_Layer())\n",
        "lr = 0.1\n",
        "net.fit(x_train, y_train, epochs=3, lr_alpha=lr, patience =5)\n",
        "net.plot(learning_rate)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}